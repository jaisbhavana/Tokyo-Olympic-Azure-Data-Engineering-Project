# Tokyo-Olympic-Azure-Data-Engineering-Project

![logo](https://github.com/jaisbhavana/Tokyo-Olympic-Azure-Data-Engineering-Project/blob/main/Doc1-1.png)

Project Description: Olympic Data Analysis with Azure Technologies
1. Introduction:
Welcome to my GitHub repository! I'm Bhavana Jaiswal, a passionate data engineering enthusiast with a background in Economics. In this repository, I'll be sharing my journey as I explore the fascinating world of data engineering, leveraging the capabilities of Azure's cutting-edge technologies. From data collection and preparation to advanced analytics, join me as I uncover insights from Olympic data using powerful tools and techniques.
 

Step 1: Data Collection from Kaggle and GitHub
Download Olympic data from Kaggle.
Upload the downloaded data into your GitHub repository.

Step 2: Creating Azure Data Factory (ADF)
Create an Azure Data Factory instance.
Define a pipeline within ADF for data movement and transformation.
Set up linked services to connect ADF with your GitHub repository.

Step 3: Data Extraction and Storage in Azure Data Lake Storage Gen 2 (ADLS)
Configure ADF to extract data from your GitHub repository.
Use the defined pipeline to move the data to ADLS Gen 2.
Organize the data in ADLS to maintain structure and accessibility.

Step 4: Data Transformation using Azure Databricks
Set up an Azure Databricks cluster.
Ingest the data from ADLS into Databricks.
Perform data transformations, cleansing, and enrichment using Databricks notebooks.
Execute advanced analytics, such as aggregations and statistical analyses.

Step 5: Storing Transformed Data Back to ADLS
Save the transformed data from Databricks back into ADLS Gen 2.
Ensure that the data is appropriately organized and labeled.

Step 6: Utilizing Azure Synapse Analytics (formerly SQL Data Warehouse)
Create an Azure Synapse Analytics instance.
Design the schema and tables to accommodate your transformed data.
Load the data from ADLS into Synapse Analytics for analysis.

Step 7: Data Exploration and Analysis
Develop complex SQL queries within Azure Synapse Analytics to explore insights.
Perform historical analysis, identify patterns, and extract meaningful information.


Key Technologies and Tools:

Azure Data Factory: Throughout this journey, we'll utilize Azure Data Factory to create automated data pipelines. This orchestration tool facilitates the seamless extraction, transformation, and loading (ETL) of Olympic data from diverse sources, optimizing data movement across various Azure services.

Azure Data Lake Storage Gen 2: To manage our data effectively, we'll leverage Azure Data Lake Storage Gen 2. This scalable and secure data lake solution serves as the repository for both raw and processed Olympic data, enabling streamlined data management.

Azure Synapse Analytics: Our analytical powerhouse, Azure Synapse Analytics, empowers us to delve into intricate querying and analysis. With this data warehousing solution, we'll uncover valuable insights, correlations, and historical patterns within the Olympic dataset.

Azure Databricks: For advanced analytics and data exploration, Azure Databricks is our tool of choice. As a collaborative Apache Spark-based platform, it enables us to perform complex data transformations, implement machine learning models, and engage in interactive data exploration.









